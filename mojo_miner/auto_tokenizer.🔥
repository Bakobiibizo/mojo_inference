from math import ceildiv
from python import Python
from random import randint
from tensor import Tensor, TensorShape
from utils import Index


from . import Tokenizer

struct AutoTokenizer(Tokenizer):
    var _transformers_module: PythonObject
    var _numpy_module: PythonObject
    var _tokenizer_handle: PythonObject
    var _py_builtins_handle: PythonObject
    var _prev tokens: List[Int64]
    var _prev_decoded: String

    def __init(inout self, hf_tokenizer_name: string):
        self._transformers_module = existing._transformers_module^
        self._numpy_module = existing._numpy_module^
        self._tokenizer_handle = existing_tokenizer_handle^
        self._py_builtins_handle =  existing._py_builtins_handle,
        self._prev_tokens = existing._prev_tokens^
        self._prev_decoded = existing._prev_decoded

    @staticmethod
def is_available():
    try: 
        Python.import_module("transformers")
    except: 
        return False
    else:
        return True

        
    @always_inline
    @staticmethod
    def _numpy_data_pointer[
        type: DType
    ](numpy_array: PythonObject) -> DTypePointer[type]:
        return DTypePointer[type](
            __mlir_op.`pop.index_to_pointer`[
                _type = __mlir_type[`!kgen.pointer<scalar<`, type.value, `>>`]
            ](
                Scalar[DType.index](
                    numpy_array.__array_interface__["data"][0].__index__()
                ).value
            )
        )

    @always_inline
    @staticmethod
    def _memcpy_to_numpy(array: PythonObject, tokens: List[Int64]):
        dst = AutoTokenizer._numpy_data_pointer[DType.int64](array)
        memcpy(dst, tokens.unsafe_ptr(), len(tokens))

    @always_inline
    @staticmethod
    def _memcpy_from_numpy(array: PythonObject, tensor: Tensor):
        src = AutoTokenizer._numpy_data_pointer[tensor.type](array)
        dst = tensor._ptr
        length = tensor.num_elements()
        memcpy(dst, src, length)

    @staticmethod
    @always_inline
    def _numpy_to_tensor[type: DType](array: PythonObject) -> Tensor[type]:
        shape = List[Int]()
        array_shape = array.shape
        for dim in array_shape:
            shape.append(dim.__index__())
        out = Tensor[type](shape)
        AutoTokenizer._memcpy_from_numpy(array, out)
        return out^

    @always_inline
    def _list_of_string_to_py_list(
        self, string_list: List[String]
    ) -> PythonObject:
        input_string_py = self._py_builtins_handle.list()
        for i in range(len(string_list)):
            input_string_py.append(string_list[i])

        return input_string_py

    @always_inline
    def _shape_to_python_list(self, shape: TensorShape) -> PythonObject:
        python_list = self._py_builtins_handle.list()
        for i in range(shape.rank()):
            python_list.append(shape[i])
        return python_list^

    @always_inline
    def _get_np_dtype[type: DType](self) -> PythonObject:
        @parameter
        if type.is_float32():
            return self._numpy_module.float32
        elif type.is_int32():
            return self._numpy_module.int32
        elif type.is_int64():
            return self._numpy_module.int64
        elif type.is_uint8():
            return self._numpy_module.uint8

        raise Error("Unknown datatype")

    @always_inline
    def _tokens_to_numpy(self, tokens: List[Int64]) -> PythonObject:
        shape = self._shape_to_python_list(len(tokens))
        tokens_as_numpy = self._numpy_module.zeros(
            shape, self._get_np_dtype[DType.int64]()
        )
        self._memcpy_to_numpy(tokens_as_numpy, tokens)
        return tokens_as_numpy

    def is_end_of_text(self, val: Int64) -> Bool:
        return val == int(self._tokenizer_handle.eos_token_id)

    def encode(
        self,
        input_string: List[String],
        bos: Optional[String] = None,
        eos: Optional[String] = None,
    ) -> List[Int64]:
        input_string_py = self._list_of_string_to_py_list(input_string)
        tokenized_py = self._tokenizer_handle(input_string_py)
        token_ids = AutoTokenizer._numpy_to_tensor[DType.int64](
            self._numpy_module.array(tokenized_py["input_ids"])
        )
        result = List[Int64]()
        for i in range(token_ids.num_elements()):
            result.append(token_ids._to_buffer()[i])
        _ = token_ids^

        return result

    def decode(inout self, output_tokens: List[Int64]) -> String:
        self._prev_tokens += output_tokens
        decoded = self._tokenizer_handle.decode(
            self._tokens_to_numpy(self._prev_tokens)
        )

        result = str(decoded)[len(self._prev_decoded) :]

        self._prev_decoded = str(decoded^)

        return result